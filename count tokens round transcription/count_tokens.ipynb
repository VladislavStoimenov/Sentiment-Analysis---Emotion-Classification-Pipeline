{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total words in first 200 lines: 1945\n",
      "üìÅ Tokenized data saved as 'tokenized_transcript.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"whisper_transcription_large.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure correct column extraction\n",
    "df[['Start Time', 'End Time', 'Transcription']] = df.iloc[:, 0].str.split(',', n=2, expand=True)\n",
    "\n",
    "# Remove unnecessary whitespace and quotes from the transcription column\n",
    "df['Transcription'] = df['Transcription'].str.strip().str.strip('\"')\n",
    "\n",
    "# Keep only the first 200 rows for analysis\n",
    "df = df.head(199)\n",
    "\n",
    "# Define a function for tokenization (removes punctuation, keeps words)\n",
    "def tokenize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "df['Tokens'] = df['Transcription'].apply(tokenize_text)\n",
    "df['Token Count (N)'] = df['Tokens'].apply(len)\n",
    "\n",
    "# Save tokenized data to a CSV file\n",
    "df[['Transcription', 'Tokens', 'Token Count (N)']].to_csv(\"tokenized_transcript.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Calculate total word count for the first 200 rows\n",
    "total_word_count = df['Token Count (N)'].sum()\n",
    "\n",
    "# Print the total token count (number of words) across the first 200 lines\n",
    "print(f\"\\n‚úÖ Total words in first 200 lines: {total_word_count}\")\n",
    "print(\"üìÅ Tokenized data saved as 'tokenized_transcript.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total words in first 200 lines (AssemblyAI file): 1664\n",
      "üìÅ Tokenized data saved as 'tokenized_assemblyai.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the new Excel file\n",
    "assemblyai_file_path = \"AssemblyAI_2.xlsx\"\n",
    "df_assemblyai = pd.read_excel(assemblyai_file_path)\n",
    "\n",
    "# Keep only the first 200 rows for analysis\n",
    "df_assemblyai = df_assemblyai.head(199)\n",
    "\n",
    "# Define a function for tokenization (removes punctuation, keeps words)\n",
    "def tokenize_assemblyai_text(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization on the \"Sentence\" column\n",
    "df_assemblyai['Tokens_AssemblyAI'] = df_assemblyai['Sentence'].apply(tokenize_assemblyai_text)\n",
    "df_assemblyai['Token_Count_AssemblyAI'] = df_assemblyai['Tokens_AssemblyAI'].apply(len)\n",
    "\n",
    "# Save tokenized data for verification\n",
    "df_assemblyai[['Sentence', 'Tokens_AssemblyAI', 'Token_Count_AssemblyAI']].to_csv(\"tokenized_assemblyai.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Calculate total word count for the first 200 rows\n",
    "total_word_count_assemblyai = df_assemblyai['Token_Count_AssemblyAI'].sum()\n",
    "\n",
    "# Print the total token count (number of words) across the first 200 lines\n",
    "print(f\"\\n‚úÖ Total words in first 200 lines (AssemblyAI file): {total_word_count_assemblyai}\")\n",
    "print(\"üìÅ Tokenized data saved as 'tokenized_assemblyai.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
